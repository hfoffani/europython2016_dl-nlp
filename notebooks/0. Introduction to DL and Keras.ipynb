{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Deep Learning and `keras`\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.    \n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n",
    "\n",
    "**Data Set Information:**\n",
    "\n",
    "The file \"sonar.mines\" contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file \"sonar.rocks\" contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock. \n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp. \n",
    "\n",
    "The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the dataset\n",
    "data = pd.read_csv(\"../data/sonar.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9  ...      51      52      53      54      55      56      57      58  \\\n",
       "0  0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084  0.0090   \n",
       "1  0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049  0.0052   \n",
       "2  0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164  0.0095   \n",
       "3  0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044  0.0040   \n",
       "4  0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048  0.0107   \n",
       "\n",
       "       59  60  \n",
       "0  0.0032   R  \n",
       "1  0.0044   R  \n",
       "2  0.0078   R  \n",
       "3  0.0117   R  \n",
       "4  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the first 5 records\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find number of rows and columns in data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['M', 'R'], dtype=object), array([111,  97]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find count of R and M in the target\n",
    "np.unique(data.ix[:,60], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "data = data.values\n",
    "X = data[:,0:60].astype(float)\n",
    "y = data[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Import keras and scikit-learn libraries.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, encoded_y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity check: If the target variable is \n",
    "#distributed similarly in both train and test.\n",
    "#If you think it isn't, you could use stratified sampling.\n",
    "#For this notebook, we will stick to what we have gotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([84, 82]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([27, 15]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(60, input_dim=60, init='normal', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1, init='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 5\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "166/166 [==============================] - 0s - loss: 0.6928 - acc: 0.4940     \n",
      "Epoch 2/100\n",
      "166/166 [==============================] - 0s - loss: 0.6916 - acc: 0.5542     \n",
      "Epoch 3/100\n",
      "166/166 [==============================] - 0s - loss: 0.6904 - acc: 0.5542     \n",
      "Epoch 4/100\n",
      "166/166 [==============================] - 0s - loss: 0.6896 - acc: 0.5181     \n",
      "Epoch 5/100\n",
      "166/166 [==============================] - 0s - loss: 0.6888 - acc: 0.5422     \n",
      "Epoch 6/100\n",
      "166/166 [==============================] - 0s - loss: 0.6879 - acc: 0.5361     \n",
      "Epoch 7/100\n",
      "166/166 [==============================] - 0s - loss: 0.6874 - acc: 0.5181     \n",
      "Epoch 8/100\n",
      "166/166 [==============================] - 0s - loss: 0.6865 - acc: 0.5663     \n",
      "Epoch 9/100\n",
      "166/166 [==============================] - 0s - loss: 0.6855 - acc: 0.5181     \n",
      "Epoch 10/100\n",
      "166/166 [==============================] - 0s - loss: 0.6846 - acc: 0.5663     \n",
      "Epoch 11/100\n",
      "166/166 [==============================] - 0s - loss: 0.6836 - acc: 0.6205     \n",
      "Epoch 12/100\n",
      "166/166 [==============================] - 0s - loss: 0.6827 - acc: 0.5783     \n",
      "Epoch 13/100\n",
      "166/166 [==============================] - 0s - loss: 0.6820 - acc: 0.6205     \n",
      "Epoch 14/100\n",
      "166/166 [==============================] - 0s - loss: 0.6810 - acc: 0.5602     \n",
      "Epoch 15/100\n",
      "166/166 [==============================] - 0s - loss: 0.6797 - acc: 0.5904     \n",
      "Epoch 16/100\n",
      "166/166 [==============================] - 0s - loss: 0.6784 - acc: 0.5964     \n",
      "Epoch 17/100\n",
      "166/166 [==============================] - 0s - loss: 0.6776 - acc: 0.6145     \n",
      "Epoch 18/100\n",
      "166/166 [==============================] - 0s - loss: 0.6765 - acc: 0.6446     \n",
      "Epoch 19/100\n",
      "166/166 [==============================] - 0s - loss: 0.6748 - acc: 0.6205     \n",
      "Epoch 20/100\n",
      "166/166 [==============================] - 0s - loss: 0.6740 - acc: 0.6265     \n",
      "Epoch 21/100\n",
      "166/166 [==============================] - 0s - loss: 0.6724 - acc: 0.6024     \n",
      "Epoch 22/100\n",
      "166/166 [==============================] - 0s - loss: 0.6715 - acc: 0.6325     \n",
      "Epoch 23/100\n",
      "166/166 [==============================] - 0s - loss: 0.6697 - acc: 0.6446     \n",
      "Epoch 24/100\n",
      "166/166 [==============================] - 0s - loss: 0.6680 - acc: 0.6566     \n",
      "Epoch 25/100\n",
      "166/166 [==============================] - 0s - loss: 0.6668 - acc: 0.6807     \n",
      "Epoch 26/100\n",
      "166/166 [==============================] - 0s - loss: 0.6654 - acc: 0.7229     \n",
      "Epoch 27/100\n",
      "166/166 [==============================] - 0s - loss: 0.6633 - acc: 0.7108     \n",
      "Epoch 28/100\n",
      "166/166 [==============================] - 0s - loss: 0.6613 - acc: 0.6928     \n",
      "Epoch 29/100\n",
      "166/166 [==============================] - 0s - loss: 0.6595 - acc: 0.6988     \n",
      "Epoch 30/100\n",
      "166/166 [==============================] - 0s - loss: 0.6576 - acc: 0.6928     \n",
      "Epoch 31/100\n",
      "166/166 [==============================] - 0s - loss: 0.6561 - acc: 0.7169     \n",
      "Epoch 32/100\n",
      "166/166 [==============================] - 0s - loss: 0.6541 - acc: 0.7108     \n",
      "Epoch 33/100\n",
      "166/166 [==============================] - 0s - loss: 0.6514 - acc: 0.7048     \n",
      "Epoch 34/100\n",
      "166/166 [==============================] - 0s - loss: 0.6492 - acc: 0.7229     \n",
      "Epoch 35/100\n",
      "166/166 [==============================] - 0s - loss: 0.6472 - acc: 0.7169     \n",
      "Epoch 36/100\n",
      "166/166 [==============================] - 0s - loss: 0.6446 - acc: 0.7108     \n",
      "Epoch 37/100\n",
      "166/166 [==============================] - 0s - loss: 0.6428 - acc: 0.6867     \n",
      "Epoch 38/100\n",
      "166/166 [==============================] - 0s - loss: 0.6397 - acc: 0.6988     \n",
      "Epoch 39/100\n",
      "166/166 [==============================] - 0s - loss: 0.6373 - acc: 0.7108     \n",
      "Epoch 40/100\n",
      "166/166 [==============================] - 0s - loss: 0.6356 - acc: 0.6928     \n",
      "Epoch 41/100\n",
      "166/166 [==============================] - 0s - loss: 0.6321 - acc: 0.6928     \n",
      "Epoch 42/100\n",
      "166/166 [==============================] - 0s - loss: 0.6297 - acc: 0.6928     \n",
      "Epoch 43/100\n",
      "166/166 [==============================] - 0s - loss: 0.6261 - acc: 0.7289     \n",
      "Epoch 44/100\n",
      "166/166 [==============================] - 0s - loss: 0.6236 - acc: 0.7349     \n",
      "Epoch 45/100\n",
      "166/166 [==============================] - 0s - loss: 0.6208 - acc: 0.7229     \n",
      "Epoch 46/100\n",
      "166/166 [==============================] - 0s - loss: 0.6175 - acc: 0.7289     \n",
      "Epoch 47/100\n",
      "166/166 [==============================] - 0s - loss: 0.6152 - acc: 0.7410     \n",
      "Epoch 48/100\n",
      "166/166 [==============================] - 0s - loss: 0.6114 - acc: 0.7229     \n",
      "Epoch 49/100\n",
      "166/166 [==============================] - 0s - loss: 0.6088 - acc: 0.7229     \n",
      "Epoch 50/100\n",
      "166/166 [==============================] - 0s - loss: 0.6051 - acc: 0.7349     \n",
      "Epoch 51/100\n",
      "166/166 [==============================] - 0s - loss: 0.6026 - acc: 0.7410     \n",
      "Epoch 52/100\n",
      "166/166 [==============================] - 0s - loss: 0.5984 - acc: 0.7289     \n",
      "Epoch 53/100\n",
      "166/166 [==============================] - 0s - loss: 0.5958 - acc: 0.7530     \n",
      "Epoch 54/100\n",
      "166/166 [==============================] - 0s - loss: 0.5912 - acc: 0.7530     \n",
      "Epoch 55/100\n",
      "166/166 [==============================] - 0s - loss: 0.5879 - acc: 0.7530     \n",
      "Epoch 56/100\n",
      "166/166 [==============================] - 0s - loss: 0.5854 - acc: 0.7289     \n",
      "Epoch 57/100\n",
      "166/166 [==============================] - 0s - loss: 0.5814 - acc: 0.7289     \n",
      "Epoch 58/100\n",
      "166/166 [==============================] - 0s - loss: 0.5802 - acc: 0.7410     \n",
      "Epoch 59/100\n",
      "166/166 [==============================] - 0s - loss: 0.5748 - acc: 0.7349     \n",
      "Epoch 60/100\n",
      "166/166 [==============================] - 0s - loss: 0.5705 - acc: 0.7410     \n",
      "Epoch 61/100\n",
      "166/166 [==============================] - 0s - loss: 0.5681 - acc: 0.7590     \n",
      "Epoch 62/100\n",
      "166/166 [==============================] - 0s - loss: 0.5634 - acc: 0.7831     \n",
      "Epoch 63/100\n",
      "166/166 [==============================] - 0s - loss: 0.5605 - acc: 0.7952     \n",
      "Epoch 64/100\n",
      "166/166 [==============================] - 0s - loss: 0.5553 - acc: 0.7410     \n",
      "Epoch 65/100\n",
      "166/166 [==============================] - 0s - loss: 0.5515 - acc: 0.7590     \n",
      "Epoch 66/100\n",
      "166/166 [==============================] - 0s - loss: 0.5494 - acc: 0.8012     \n",
      "Epoch 67/100\n",
      "166/166 [==============================] - 0s - loss: 0.5451 - acc: 0.7952     \n",
      "Epoch 68/100\n",
      "166/166 [==============================] - 0s - loss: 0.5432 - acc: 0.7771     \n",
      "Epoch 69/100\n",
      "166/166 [==============================] - 0s - loss: 0.5394 - acc: 0.7892     \n",
      "Epoch 70/100\n",
      "166/166 [==============================] - 0s - loss: 0.5362 - acc: 0.7410     \n",
      "Epoch 71/100\n",
      "166/166 [==============================] - 0s - loss: 0.5331 - acc: 0.7771     \n",
      "Epoch 72/100\n",
      "166/166 [==============================] - 0s - loss: 0.5298 - acc: 0.8012     \n",
      "Epoch 73/100\n",
      "166/166 [==============================] - 0s - loss: 0.5237 - acc: 0.8012     \n",
      "Epoch 74/100\n",
      "166/166 [==============================] - 0s - loss: 0.5237 - acc: 0.7651     \n",
      "Epoch 75/100\n",
      "166/166 [==============================] - 0s - loss: 0.5190 - acc: 0.7831     \n",
      "Epoch 76/100\n",
      "166/166 [==============================] - 0s - loss: 0.5169 - acc: 0.7831     \n",
      "Epoch 77/100\n",
      "166/166 [==============================] - 0s - loss: 0.5132 - acc: 0.7952     \n",
      "Epoch 78/100\n",
      "166/166 [==============================] - 0s - loss: 0.5074 - acc: 0.7892     \n",
      "Epoch 79/100\n",
      "166/166 [==============================] - 0s - loss: 0.5056 - acc: 0.7892     \n",
      "Epoch 80/100\n",
      "166/166 [==============================] - 0s - loss: 0.5034 - acc: 0.8072     \n",
      "Epoch 81/100\n",
      "166/166 [==============================] - 0s - loss: 0.4977 - acc: 0.7831     \n",
      "Epoch 82/100\n",
      "166/166 [==============================] - 0s - loss: 0.4979 - acc: 0.7952     \n",
      "Epoch 83/100\n",
      "166/166 [==============================] - 0s - loss: 0.4956 - acc: 0.7831     \n",
      "Epoch 84/100\n",
      "166/166 [==============================] - 0s - loss: 0.4923 - acc: 0.7831     \n",
      "Epoch 85/100\n",
      "166/166 [==============================] - 0s - loss: 0.4879 - acc: 0.8072     \n",
      "Epoch 86/100\n",
      "166/166 [==============================] - 0s - loss: 0.4849 - acc: 0.8313     \n",
      "Epoch 87/100\n",
      "166/166 [==============================] - 0s - loss: 0.4823 - acc: 0.7771     \n",
      "Epoch 88/100\n",
      "166/166 [==============================] - 0s - loss: 0.4796 - acc: 0.8373     \n",
      "Epoch 89/100\n",
      "166/166 [==============================] - 0s - loss: 0.4794 - acc: 0.7651     \n",
      "Epoch 90/100\n",
      "166/166 [==============================] - 0s - loss: 0.4764 - acc: 0.7892     \n",
      "Epoch 91/100\n",
      "166/166 [==============================] - 0s - loss: 0.4733 - acc: 0.8193     \n",
      "Epoch 92/100\n",
      "166/166 [==============================] - 0s - loss: 0.4713 - acc: 0.7771     \n",
      "Epoch 93/100\n",
      "166/166 [==============================] - 0s - loss: 0.4667 - acc: 0.8193     \n",
      "Epoch 94/100\n",
      "166/166 [==============================] - 0s - loss: 0.4688 - acc: 0.7952     \n",
      "Epoch 95/100\n",
      "166/166 [==============================] - 0s - loss: 0.4665 - acc: 0.8072     \n",
      "Epoch 96/100\n",
      "166/166 [==============================] - 0s - loss: 0.4617 - acc: 0.7952     \n",
      "Epoch 97/100\n",
      "166/166 [==============================] - 0s - loss: 0.4594 - acc: 0.8072     \n",
      "Epoch 98/100\n",
      "166/166 [==============================] - 0s - loss: 0.4573 - acc: 0.8133     \n",
      "Epoch 99/100\n",
      "166/166 [==============================] - 0s - loss: 0.4589 - acc: 0.7892     \n",
      "Epoch 100/100\n",
      "166/166 [==============================] - 0s - loss: 0.4558 - acc: 0.7831     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x108bcc2d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch=nb_epoch, \n",
    "          batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 5/42 [==>...........................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.51872508227825165, 0.71428573131561279]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In the below code \n",
    "# - add another dense layer - with 30 neurons. \n",
    "# - Set the activation to sigmoid\n",
    "# - Observe what happens if you set the activation to relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modify this code ! Add one more layer with activation sigmoid\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=60, init='normal', activation='sigmoid'))\n",
    "model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, nb_epoch=nb_epoch, \n",
    "          batch_size=batch_size, verbose=verbose)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modify this code ! Add one more layer with activation relu\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=60, init='normal', activation='relu'))\n",
    "model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, nb_epoch=nb_epoch, \n",
    "          batch_size=batch_size, verbose=verbose)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
